# MaxIOFS - Development Roadmap

**Version**: 0.9.2-beta
**Last Updated**: February 24, 2026
**Status**: Beta - S3 Core 100% Compatible

## üìä Project Status

| Metric | Value | Notes |
|--------|-------|-------|
| S3 Core API | 100% | All standard S3 operations |
| Backend Coverage | ~75% | At practical ceiling ‚Äî see details below |
| Frontend Coverage | 100% | Complete |
| Production Ready | Testing | Target: Q4 2026 |

### Backend Test Coverage Reality (February 7, 2026)

| Module | Coverage | Notes |
|--------|----------|-------|
| internal/metadata | 87.4% | Remaining ~13% are Pebble internal error branches (WAL failures, I/O errors) ‚Äî not simulable in unit tests |
| internal/object | 77.3% | Remaining gaps: `NewManager` init (47.8%), `GetObject` encryption/range branches (53.7%), `cleanupEmptyDirectories` (34.6%), `deleteSpecificVersion` blocked by Windows file-locking bug |
| cmd/maxiofs | 71.4% | `main()` is 0% (entrypoint, expected), `runServer` at 87.5% |
| internal/server | 66.1% | `Start/startAPIServer/startConsoleServer/shutdown` are 0% (HTTP server lifecycle, not unit-testable). Cluster handlers (30-55%) require real remote nodes. Migration/replication handlers need live infrastructure |
| internal/replication | 19.0% | CRUD rule management tested. `s3client.go`, `worker.go`, `adapter.go` are all 0% ‚Äî they operate against real remote S3 endpoints and cannot be unit-tested without full network infrastructure |

**Conclusion**: All testable business logic has been covered. The remaining uncovered code falls into categories that cannot be meaningfully unit-tested: server lifecycle, remote node communication, filesystem-level operations, and low-level database error branches. Reaching 90%+ would require integration test infrastructure (multi-node cluster, remote S3 endpoints) which is outside the scope of unit testing.

---

## üî¥ CRITICAL (Security Hardening)

### 1. JWT Signature Verification (CRITICAL) ‚úÖ
- [x] `parseBasicToken()` now verifies HMAC-SHA256 signature before trusting payload using `hmac.Equal()` for constant-time comparison
- **File**: `internal/auth/manager.go:1100-1132`
- **Tests**: `TestValidateJWT_ForgedSignature` (4 cases), `TestValidateJWT_TamperedPayload` ‚Äî all pass

### 2. CORS Wildcard Removal (CRITICAL) ‚úÖ
- [x] Replaced hardcoded `Access-Control-Allow-Origin: *` with proper `middleware.CORSWithConfig()` using origin validation
- **File**: `internal/server/console_api.go`
- **Tests**: Existing CORS middleware tests cover origin validation (disallowed origins, wildcards, custom validators)

### 3. Rate Limiting IP Spoofing (CRITICAL) ‚úÖ
- [x] `IPKeyExtractor` now only trusts `X-Forwarded-For`/`X-Real-IP` when request comes from a trusted proxy. Added `TrustedProxies`, `stripPort()`, `isTrustedProxy()`.
- **File**: `internal/middleware/ratelimit.go:248-301`
- **Tests**: `TestIPKeyExtractor` (7 cases), `TestStripPort` (7 cases), `TestIsTrustedProxy` (2 cases) ‚Äî all pass

---

## üü† HIGH (Stability & Robustness)

### 4. Default Password Change Notification ‚úÖ
- [x] Backend returns `default_password: true` in login response when admin/admin is used
- [x] Frontend shows persistent security warning in notification bell with amber icon, links to user profile
- [x] Warning clears automatically when password is changed via `APIClient.changePassword()`
- **Files**: `internal/server/console_api.go`, `web/frontend/src/lib/api.ts`, `web/frontend/src/components/layout/AppLayout.tsx`

### 5. Goroutine Leak in Decryption Pipeline ‚úÖ
- [x] Added context cancellation monitoring ‚Äî when caller abandons the reader, `ctx.Done()` triggers `pipeWriter.CloseWithError()`, unblocking the goroutine
- **File**: `internal/object/manager.go:318-340`

### 6. Unbounded Map Growth in Replication Manager ‚úÖ
- [x] `DeleteRule()` now cleans up `ruleLocks` entry for deleted rules
- [x] `processScheduledRules()` now cleans up `lastSync` entries for rules no longer in the database
- **File**: `internal/replication/manager.go`

### 7. Race Condition in Cluster Cache ‚úÖ (False Positive)
- [x] Verified: all `c.entries` accesses are correctly protected with `sync.RWMutex` ‚Äî `RLock` for reads, `Lock` for writes
- **File**: `internal/cluster/cache.go` ‚Äî no changes needed

### 8. Unchecked `crypto/rand.Read` Error ‚úÖ
- [x] Added error check ‚Äî falls back to timestamp-only version ID if `crypto/rand` fails
- **File**: `internal/object/manager.go:207`

### 9. Array Bounds Check in S3 Signature Parsing ‚úÖ (False Positive)
- [x] Verified: all array accesses in `parseS3SignatureV4` and `parseS3SignatureV2` already have proper bounds checks (`len(credParts) >= 2`, `len(kv) != 2`, `len(parts) != 2`)
- **File**: `internal/auth/manager.go:1189-1212` ‚Äî no changes needed

---

## üü° MEDIUM PRIORITY

### Code Quality
- [x] HTTP response body not always closed via `defer` immediately after assignment ‚Äî Verified: all `resp.Body.Close()` are properly deferred (false positive)
- [x] Audit logging errors silently ignored in 12 locations in `console_api.go` ‚Äî Added `logAuditEvent()` helper that logs warnings on failure, migrated all 12 call sites
- [x] Temp file handle leak potential in `internal/object/manager.go:368-383` ‚Äî Added `defer tempFile.Close()` immediately after creation to ensure cleanup on panic
- [x] Tag index deletion error ignored in `internal/metadata/pebble_objects.go` ‚Äî Now returns error on failed batch delete to prevent inconsistent state
- [x] Path traversal with URL-encoded `%2e%2e%2f` ‚Äî Verified safe: Go's `net/http` decodes URL-encoded paths before handlers, `strings.Contains(path, "..")` catches decoded traversal, and `filepath.Join` normalizes as defense-in-depth

---

## üü† HIGH ‚Äî Cluster Resilience: Stale Node & Network Partition (v0.9.2-beta)

Two distinct failure scenarios require proper handling. Both involve a node that was isolated
from the cluster for a period and then reconnects. The current sync system has no conflict
resolution and simply overwrites ‚Äî which causes entity resurrection and data loss.

---

### Scenario A ‚Äî Node offline (clean shutdown or crash, no client traffic)

The cluster is the sole source of truth. The returning node just needs to pull the current
authoritative state and discard anything that was deleted during its absence.

### Scenario B ‚Äî Network partition (node isolated but alive, serving clients)

Both sides diverge independently. Neither is authoritative. Requires bidirectional merge with
conflict resolution. This is the harder and more dangerous case.

```
Detection key:
  last_local_write_at ‚â§ last_seen_at_shutdown  ‚Üí  Scenario A (offline, no local writes)
  last_local_write_at >  last_seen_at_shutdown  ‚Üí  Scenario B (partition, had local writes)
```

---

### Phase 1 ‚Äî Schema & Detection  [x] ‚úÖ

**Files**: `internal/cluster/schema.go`

- [x] Column `is_stale BOOLEAN NOT NULL DEFAULT 0` on `cluster_nodes`
- [x] Column `last_local_write_at TIMESTAMP` on `cluster_nodes`
- [x] `applyStaleNodeMigration()` adds both columns to existing databases

---

### Phase 2 ‚Äî Stale Detection in Health Checker  [x] ‚úÖ

**File**: `internal/cluster/health.go`

- [x] `CheckNodeHealth()` marks `is_stale = true` when node reconnects after `> StalenessThreshold` (7 days)
- [x] `checkAndMarkStale()` helper
- [x] `touchLocalWriteAt()` helper in `internal/server/cluster_write_tracking.go`

---

### Phase 3 ‚Äî LWW (Last-Write-Wins) in all entity upsert handlers  [x] ‚úÖ

**File**: `internal/server/cluster_object_handlers.go`, `internal/server/cluster_tenant_handlers.go`

- [x] Tenants ‚Äî LWW on `updated_at` (TIMESTAMP type)
- [x] Users ‚Äî LWW on `updated_at` (int64 Unix seconds)
- [x] IDP Providers ‚Äî LWW on `updated_at` (int64 Unix seconds)
- [x] Group Mappings ‚Äî LWW on `updated_at` (int64 Unix seconds)
- [x] Access Keys ‚Äî implicit LWW via `created_at` in stale reconciler snapshot comparison (no `updated_at` column in schema); `INSERT OR REPLACE` only executes for entities absent from the peer
- [x] Bucket Permissions ‚Äî same as access keys (`granted_at` used as timestamp proxy)

---

### Phase 4 ‚Äî Tombstone vs Entity timestamp comparison  [x] ‚úÖ

**Files**: `internal/cluster/deletion_log.go`, `internal/server/cluster_object_handlers.go`, `internal/server/cluster_tenant_handlers.go`

- [x] `EntityIsNewerThanTombstone()` in `deletion_log.go` ‚Äî supports Tenant, User, IDPProvider, GroupMapping (full `updated_at`); returns false for AccessKey/BucketPermission (no `updated_at` ‚Äî tombstone always wins)
- [x] `handleReceiveTenantDeleteSync` ‚Äî Phase 4 check present
- [x] `handleReceiveUserDeleteSync` ‚Äî Phase 4 check present
- [x] `handleReceiveIDPProviderDeleteSync` ‚Äî Phase 4 check present
- [x] `handleReceiveGroupMappingDeleteSync` ‚Äî Phase 4 check present
- [x] `handleReceiveAccessKeyDeleteSync` ‚Äî Phase 4 check present (always false; tombstone wins by design)
- [x] `handleReceiveBucketPermissionDeleteSync` ‚Äî Phase 4 check present (always false; tombstone wins by design)
- [x] `handleReceiveDeletionLogSync` ‚Äî Phase 4 check in bulk tombstone application loop

---

### Phase 5 ‚Äî State Snapshot Endpoint  [x] ‚úÖ

**Files**: `internal/cluster/snapshot.go`, `internal/server/cluster_snapshot_handler.go`

- [x] `GET /api/internal/cluster/state-snapshot` ‚Äî HMAC-authenticated, returns `StateSnapshot`
- [x] `BuildLocalSnapshot()` queries all 6 entity types + deletion log
- [x] `fetchRemoteSnapshot()` in `stale_reconciler.go` as the client-side caller

---

### Phase 6 ‚Äî Stale Reconciler  [x] ‚úÖ

**File**: `internal/cluster/stale_reconciler.go`

- [x] `ModeOffline` / `ModePartition` detection via `last_local_write_at`
- [x] `reconcileWithPeer()` ‚Äî fetches remote snapshot, pushes locally-newer entities (ModePartition), syncs tombstones bidirectionally (both modes)
- [x] `pushNewerEntities()` ‚Äî all 6 entity types via `newerStamps()` + per-entity push methods
- [x] `pushTombstonesToPeer()` + `applyRemoteTombstones()` ‚Äî bidirectional tombstone sync
- [x] `clearStaleFlag()` ‚Äî resets `is_stale=0` and `last_local_write_at=NULL` on completion

---

### Phase 7 ‚Äî Integration into Server Startup  [x] ‚úÖ

**File**: `internal/server/server.go`

- [x] `staleReconciler *cluster.StaleReconciler` field on `Server` struct (line 72)
- [x] `NewStaleReconciler()` called in server initialization (line 351)
- [x] `staleReconciler.Reconcile(ctx)` called at startup when cluster is enabled (line 506)

---

### Phase 8 ‚Äî Track `last_local_write_at`  [x] ‚úÖ

**File**: `internal/server/cluster_write_tracking.go`, `internal/server/console_api.go`, `internal/server/console_idp.go`

- [x] `touchLocalWriteAt(ctx)` helper ‚Äî updates `last_local_write_at` on local node row
- [x] Called in all 10 entity write handlers in `console_api.go` (createTenant, updateTenant, createUser, updateUser, createAccessKey, grantBucketPermission, revokeBucketPermission, ...)
- [x] Called in all 6 IDP/group-mapping handlers in `console_idp.go` (createIDP, updateIDP, deleteIDP, createGroupMapping, updateGroupMapping, deleteGroupMapping)

---

### Phase 9 ‚Äî Tests  [x] ‚úÖ

**File**: `internal/cluster/stale_reconciler_test.go` ‚Äî 25 tests, all passing

- [x] `TestNewerStamps` (6 sub-tests) ‚Äî pure function: absent/newer included, equal/older skipped, mixed batch, empty input
- [x] `TestBuildStampIndex` ‚Äî index built correctly for all 6 entity types
- [x] `TestBuildLocalSnapshot_Empty` ‚Äî empty DB produces empty snapshot
- [x] `TestBuildLocalSnapshot_WithEntities` ‚Äî all 6 entity types + tombstones appear in snapshot with correct timestamps
- [x] `TestEntityIsNewerThanTombstone` (11 sub-tests) ‚Äî tenant/user/IDP/group-mapping: older=false, newer=true; access-key/bucket-permission: always false; entity-not-found=false
- [x] `TestDetectMode` (2 sub-tests) ‚Äî NULL last_local_write_at ‚Üí ModeOffline; set ‚Üí ModePartition
- [x] `TestApplyRemoteTombstones` (4 sub-tests) ‚Äî new tombstone recorded; equal/newer local tombstone skipped; entity newer than tombstone skipped (LWW); mixed batch filters correctly
- [x] `TestReconcile_SkipsWhenNotStale` ‚Äî node not stale ‚Üí returns nil immediately
- [x] `TestReconcile_SkipsWhenNoPeers` ‚Äî no peers ‚Üí stale flag remains set
- [x] `TestReconcile_ClearsStaleFlag` ‚Äî is_stale=0 and last_local_write_at=NULL after reconciliation
- [x] `TestReconcile_ModeOffline_FetchesSnapshot` ‚Äî state-snapshot endpoint called on peer
- [x] `TestReconcile_ModeOffline_DoesNotPushEntities` ‚Äî no entity sync endpoints called in ModeOffline
- [x] `TestReconcile_ModePartition_PushesLocallyNewerEntities` ‚Äî user-sync called when local is newer
- [x] `TestReconcile_ModePartition_SkipsRemoteNewerEntities` ‚Äî no push when remote is strictly newer (LWW)
- [x] `TestReconcile_PushesLocalTombstonesToPeer` ‚Äî local tombstones sent to peer via deletion-log-sync
- [x] `TestReconcile_AppliesRemoteTombstonesLocally` ‚Äî remote tombstones recorded in local deletion log

---

### Summary of files to create / modify

| File | Action |
|---|---|
| `internal/cluster/schema.go` | Add `is_stale`, `last_local_write_at` columns |
| `internal/cluster/migration.go` | New migration for the 2 new columns |
| `internal/cluster/health.go` | Detect stale on reconnect, set `is_stale = true` |
| `internal/cluster/manager.go` | `SetNodeStale`, `IsNodeStale`, `UpdateLastLocalWriteAt`, `GetLocalNodeLastLocalWriteAt` |
| `internal/cluster/deletion_log.go` | Tombstone timestamp comparison before applying |
| `internal/cluster/stale_reconciler.go` | **New** ‚Äî full reconciliation logic |
| `internal/cluster/stale_reconciler_test.go` | **New** ‚Äî all unit tests |
| `internal/server/cluster_handlers.go` | New `GET /api/internal/cluster/state-snapshot` endpoint |
| `internal/server/server.go` | Startup: check stale ‚Üí run reconciler before sync managers |
| `internal/server/console_api.go` | All entity handlers call `UpdateLastLocalWriteAt` |

---

## üü° PENDING ‚Äî v0.9.2-beta

### 1. Maintenance Mode Enforcement ‚úÖ

- [x] S3 middleware: PUT/POST/DELETE blocked with 503 + XML error; GET/HEAD pass through
- [x] Console API middleware: mutating requests blocked with 503 JSON `MAINTENANCE_MODE`; exempt: `/auth/`, `/health`, `/settings`, `/api/internal/`, `/notifications`
- [x] Frontend amber banner in AppLayout, reactive without page reload via `queryClient.invalidateQueries(['serverConfig'])`
- [x] `handleGetServerConfig` includes `maintenanceMode: bool`
- **Files**: `internal/middleware/maintenance.go`, `internal/server/console_api.go`, `internal/server/server.go`, `web/frontend/src/components/layout/AppLayout.tsx`

---

### 2. Disk Space Threshold Alerts ‚úÖ

- [x] Settings: `system.disk_warning_threshold` (80%), `system.disk_critical_threshold` (90%)
- [x] `internal/server/disk_alerts.go`: goroutine every 5 min, `diskAlertState` deduplication
- [x] SSE to global admins + email to all active global admin accounts with email
- [x] SMTP: `internal/email/sender.go`, `email.*` settings category, test email endpoint `POST /settings/email/test`
- [x] Frontend Email tab in Settings with Test Email button
- **Files**: `internal/email/sender.go`, `internal/server/disk_alerts.go`, `internal/settings/manager.go`, `web/frontend/src/pages/settings/index.tsx`

---

### 3. Tenant Quota Warning Notifications ‚úÖ

- [x] Callback `SetStorageQuotaAlertCallback` added to auth Manager interface + `authManager` struct
- [x] `IncrementTenantStorage` fires callback asynchronously after every successful increment
- [x] `internal/server/quota_alerts.go`: `quotaAlertTracker` with per-tenant `sync.Map` deduplication
- [x] SSE to tenant admins + global admins; email to both groups
- [x] Tenants with `MaxStorageBytes = 0` (unlimited) skipped
- [x] Frontend: storage bar thresholds aligned to 80% (amber) / 90% (red) with inline label
- **Files**: `internal/auth/manager.go`, `internal/server/quota_alerts.go`, `internal/server/server.go`, `web/frontend/src/pages/tenants/index.tsx`

---

### 4. Object Integrity Verification (MEDIUM)

**Status**: MD5 is computed at write time and stored as ETag in Pebble. Never re-verified after storage.

- [ ] `VerifyObjectIntegrity(ctx, bucketPath, objectKey) error` in `internal/object/manager.go`: reads the object file from disk, computes MD5, compares with stored ETag ‚Äî returns error on mismatch
- [ ] Background scrubber goroutine (`startIntegrityScrubber`): runs once every 24 hours, iterates all objects via `ListObjects`, calls `VerifyObjectIntegrity` for each, logs corrupted objects as `logrus.Error` and records an audit event (`EventTypeDataCorruption`)
- [ ] New audit event type `EventTypeDataCorruption` with fields: bucket, object key, expected ETag, detected ETag, file path
- [ ] Admin endpoint `POST /buckets/{bucket}/verify-integrity` ‚Äî triggers an on-demand scan for a specific bucket, returns count of objects checked and list of corrupted objects found
- [ ] Skip objects with empty ETag (delete markers, multipart in-progress)
- [ ] Skip encrypted objects where ETag is of the unencrypted content (verify using `original-etag` from storage metadata)

---

### 5. Operational Documentation (LOW)

**Status**: Technical docs exist (`ARCHITECTURE.md`, `CLUSTER.md`, `SECURITY.md`). No operator runbook.

- [ ] `docs/OPERATIONS.md` ‚Äî runbook for production operators:
  - What to do when a cluster node goes down
  - How to safely remove a node from the cluster
  - How to recover from a Pebble crash (WAL recovery is automatic, but document the indicators)
  - How to interpret audit logs for security incidents
  - Recommended monitoring alerts for Prometheus/Grafana
  - Disk space management (what to do when approaching capacity)

---

## ‚úÖ COMPLETED

### v0.9.2-beta (February 2026)
- ‚úÖ Maintenance Mode enforcement ‚Äî S3 + Console API middleware, reactive frontend banner
- ‚úÖ Disk space alerts ‚Äî SSE + SMTP email to global admins when disk crosses 80%/90%; test email endpoint
- ‚úÖ Tenant quota warnings ‚Äî SSE + email on 80%/90% threshold crossing; per-tenant deduplication; colored storage bar in UI
- ‚úÖ Replaced BadgerDB with Pebble (CockroachDB's LSM-tree engine) for all S3 object/bucket metadata ‚Äî crash-safe WAL eliminates MANIFEST corruption on unclean shutdown
- ‚úÖ Transparent auto-migration: `MigrateFromBadgerIfNeeded()` detects `metadata/KEYREGISTRY`, migrates all keys to Pebble in batches, renames directories atomically ‚Äî no user intervention
- ‚úÖ Decoupled ACL, bucket, object, metrics, notifications from BadgerDB via `metadata.RawKVStore` interface
- ‚úÖ Multipart TTL replaced with hourly cleanup goroutine (Pebble has no native TTL)
- ‚úÖ All test files updated to use `PebbleStore`; migration corrected existing under-reported counters
- ‚úÖ Cluster: `StateSnapshot` endpoint + `StaleReconciler` (LWW conflict resolution on reconnect)
- ‚úÖ Cluster: Write tracking (`last_local_write_at`) for accurate partition detection
- ‚úÖ Cluster: Phase 3 LWW complete for all 6 entity upsert handlers (tenants, users, IDP providers, group mappings use `updated_at`; access keys and bucket permissions use `created_at`/`granted_at` as timestamp proxy in snapshot comparison)
- ‚úÖ Cluster: Phase 4 `EntityIsNewerThanTombstone` check added to all 6 delete handlers ‚Äî `handleReceiveAccessKeyDeleteSync` and `handleReceiveBucketPermissionDeleteSync` were the last two missing
- ‚úÖ Cluster: Phase 9 stale reconciler tests ‚Äî 25 tests covering pure functions (newerStamps, buildStampIndex), DB logic (BuildLocalSnapshot, EntityIsNewerThanTombstone, detectMode, applyRemoteTombstones), and full HTTP integration (Reconcile with mock peer server)
- ‚úÖ Bucket metrics under-reported under concurrent load ‚Äî replaced `UpdateBucketMetrics` OCC retry loop (5 attempts) with per-bucket `sync.Mutex` via `sync.Map`. Resolves VEEAM 4.2 GB stored / 2.21 GB shown discrepancy.
- ‚úÖ `RecalculateBucketStats` tenant prefix fix ‚Äî was scanning `obj:bucketName:` instead of `obj:tenantID/bucketName:key`, always returning 0 for tenant buckets.
- ‚úÖ Admin endpoint `POST /buckets/{bucket}/recalculate-stats` ‚Äî full Pebble scan to resync counters on demand.
- ‚úÖ Background stats reconciler ‚Äî goroutine that runs `RecalculateBucketStats` for all buckets every 15 minutes, 2-minute initial delay, clean shutdown on context cancellation.
- ‚úÖ Frontend dynamic refresh ‚Äî `refetchInterval: 30000` on dashboard, buckets listing, and bucket detail pages so stats update without navigation.
- ‚úÖ Removed `TestHandleTestLogOutput` ‚Äî called non-existent `server.handleTestLogOutput`, caused compilation failure.

### v0.9.1-beta (February 2026)
- ‚úÖ Tombstone-based cluster deletion sync ‚Äî new `cluster_deletion_log` table, `DeletionLogSyncManager`, tombstone checks in all upsert handlers
- ‚úÖ IDP provider and group mapping cluster sync with automatic synchronization
- ‚úÖ Delete-sync endpoints for all 6 entity types (users, tenants, access keys, bucket permissions, IDP providers, group mappings)
- ‚úÖ 36 new cluster sync tests (deletion log, IDP provider sync, group mapping sync)

### v0.8.0-beta (February 2026)
- ‚úÖ Object Filters & Advanced Search (content-type, size range, date range, tags) ‚Äî new `/objects/search` endpoint + frontend filter panel
- ‚úÖ Version check notification badge for global admins (proxied through backend)
- ‚úÖ Dark mode toggle fixed ‚Äî now uses ThemeContext, persists to user profile
- ‚úÖ CI/CD test fix ‚Äî `TestCPUStats_ConsistentData` frequency variance threshold increased for virtualized environments

### v0.7.0-beta (January-February 2026)
- ‚úÖ Bucket Inventory System
- ‚úÖ Database Migration System
- ‚úÖ Performance Profiling & Benchmarking
- ‚úÖ Cluster Production Hardening (rate limiting, circuit breakers, metrics)
- ‚úÖ ListBuckets cross-node aggregation (fixed UX blocker)
- ‚úÖ Cluster-aware quota enforcement (fixed security vulnerability)
- ‚úÖ Backend test coverage expansion ‚Äî reached practical ceiling (metadata 87.4%, object 77.3%, server 66.1%, cmd 71.4%)

### v0.6.x
- ‚úÖ Cluster Management System
- ‚úÖ Performance Metrics & Dashboards
- ‚úÖ Prometheus/Grafana Integration
- ‚úÖ Bucket Replication

### v0.5.0 and earlier
- ‚úÖ Core S3 Operations (PutObject, GetObject, DeleteObject, ListObjects)
- ‚úÖ Bucket Versioning & Lifecycle Policies
- ‚úÖ Object Lock & Retention (COMPLIANCE/GOVERNANCE)
- ‚úÖ Server-side Encryption (AES-256-CTR)
- ‚úÖ Multi-tenancy with quotas
- ‚úÖ Two-Factor Authentication
- ‚úÖ Bucket Policies, ACLs, CORS, Tags

---

## üìù References

- Changelog: [CHANGELOG.md](CHANGELOG.md)
- API Documentation: [docs/API.md](docs/API.md)
- Cluster Guide: [docs/CLUSTER.md](docs/CLUSTER.md)
- Performance: [docs/PERFORMANCE.md](docs/PERFORMANCE.md)
