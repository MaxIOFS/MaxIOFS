# MaxIOFS - Development Roadmap

**Version**: 0.9.2-beta
**Last Updated**: February 23, 2026
**Status**: Beta - S3 Core 100% Compatible

## üìä Project Status

| Metric | Value | Notes |
|--------|-------|-------|
| S3 Core API | 100% | All standard S3 operations |
| Backend Coverage | ~75% | At practical ceiling ‚Äî see details below |
| Frontend Coverage | 100% | Complete |
| Production Ready | Testing | Target: Q4 2026 |

### Backend Test Coverage Reality (February 7, 2026)

| Module | Coverage | Notes |
|--------|----------|-------|
| internal/metadata | 87.4% | Remaining ~13% are BadgerDB internal error branches (transaction failures, corruption) ‚Äî not simulable in unit tests |
| internal/object | 77.3% | Remaining gaps: `NewManager` init (47.8%), `GetObject` encryption/range branches (53.7%), `cleanupEmptyDirectories` (34.6%), `deleteSpecificVersion` blocked by Windows file-locking bug |
| cmd/maxiofs | 71.4% | `main()` is 0% (entrypoint, expected), `runServer` at 87.5% |
| internal/server | 66.1% | `Start/startAPIServer/startConsoleServer/shutdown` are 0% (HTTP server lifecycle, not unit-testable). Cluster handlers (30-55%) require real remote nodes. Migration/replication handlers need live infrastructure |
| internal/replication | 19.0% | CRUD rule management tested. `s3client.go`, `worker.go`, `adapter.go` are all 0% ‚Äî they operate against real remote S3 endpoints and cannot be unit-tested without full network infrastructure |

**Conclusion**: All testable business logic has been covered. The remaining uncovered code falls into categories that cannot be meaningfully unit-tested: server lifecycle, remote node communication, filesystem-level operations, and low-level database error branches. Reaching 90%+ would require integration test infrastructure (multi-node cluster, remote S3 endpoints) which is outside the scope of unit testing.

---

## üî¥ CRITICAL (Security Hardening)

### 1. JWT Signature Verification (CRITICAL) ‚úÖ
- [x] `parseBasicToken()` now verifies HMAC-SHA256 signature before trusting payload using `hmac.Equal()` for constant-time comparison
- **File**: `internal/auth/manager.go:1100-1132`
- **Tests**: `TestValidateJWT_ForgedSignature` (4 cases), `TestValidateJWT_TamperedPayload` ‚Äî all pass

### 2. CORS Wildcard Removal (CRITICAL) ‚úÖ
- [x] Replaced hardcoded `Access-Control-Allow-Origin: *` with proper `middleware.CORSWithConfig()` using origin validation
- **File**: `internal/server/console_api.go`
- **Tests**: Existing CORS middleware tests cover origin validation (disallowed origins, wildcards, custom validators)

### 3. Rate Limiting IP Spoofing (CRITICAL) ‚úÖ
- [x] `IPKeyExtractor` now only trusts `X-Forwarded-For`/`X-Real-IP` when request comes from a trusted proxy. Added `TrustedProxies`, `stripPort()`, `isTrustedProxy()`.
- **File**: `internal/middleware/ratelimit.go:248-301`
- **Tests**: `TestIPKeyExtractor` (7 cases), `TestStripPort` (7 cases), `TestIsTrustedProxy` (2 cases) ‚Äî all pass

---

## üü† HIGH (Stability & Robustness)

### 4. Default Password Change Notification ‚úÖ
- [x] Backend returns `default_password: true` in login response when admin/admin is used
- [x] Frontend shows persistent security warning in notification bell with amber icon, links to user profile
- [x] Warning clears automatically when password is changed via `APIClient.changePassword()`
- **Files**: `internal/server/console_api.go`, `web/frontend/src/lib/api.ts`, `web/frontend/src/components/layout/AppLayout.tsx`

### 5. Goroutine Leak in Decryption Pipeline ‚úÖ
- [x] Added context cancellation monitoring ‚Äî when caller abandons the reader, `ctx.Done()` triggers `pipeWriter.CloseWithError()`, unblocking the goroutine
- **File**: `internal/object/manager.go:318-340`

### 6. Unbounded Map Growth in Replication Manager ‚úÖ
- [x] `DeleteRule()` now cleans up `ruleLocks` entry for deleted rules
- [x] `processScheduledRules()` now cleans up `lastSync` entries for rules no longer in the database
- **File**: `internal/replication/manager.go`

### 7. Race Condition in Cluster Cache ‚úÖ (False Positive)
- [x] Verified: all `c.entries` accesses are correctly protected with `sync.RWMutex` ‚Äî `RLock` for reads, `Lock` for writes
- **File**: `internal/cluster/cache.go` ‚Äî no changes needed

### 8. Unchecked `crypto/rand.Read` Error ‚úÖ
- [x] Added error check ‚Äî falls back to timestamp-only version ID if `crypto/rand` fails
- **File**: `internal/object/manager.go:207`

### 9. Array Bounds Check in S3 Signature Parsing ‚úÖ (False Positive)
- [x] Verified: all array accesses in `parseS3SignatureV4` and `parseS3SignatureV2` already have proper bounds checks (`len(credParts) >= 2`, `len(kv) != 2`, `len(parts) != 2`)
- **File**: `internal/auth/manager.go:1189-1212` ‚Äî no changes needed

---

## üü° MEDIUM PRIORITY

### Code Quality
- [x] HTTP response body not always closed via `defer` immediately after assignment ‚Äî Verified: all `resp.Body.Close()` are properly deferred (false positive)
- [x] Audit logging errors silently ignored in 12 locations in `console_api.go` ‚Äî Added `logAuditEvent()` helper that logs warnings on failure, migrated all 12 call sites
- [x] Temp file handle leak potential in `internal/object/manager.go:368-383` ‚Äî Added `defer tempFile.Close()` immediately after creation to ensure cleanup on panic
- [x] Tag index deletion error ignored in `internal/metadata/badger_objects.go:563` ‚Äî Now returns error on failed `txn.Delete()` to prevent inconsistent state
- [x] Path traversal with URL-encoded `%2e%2e%2f` ‚Äî Verified safe: Go's `net/http` decodes URL-encoded paths before handlers, `strings.Contains(path, "..")` catches decoded traversal, and `filepath.Join` normalizes as defense-in-depth

---

## üü† HIGH ‚Äî Cluster Resilience: Stale Node & Network Partition (v0.9.2-beta)

Two distinct failure scenarios require proper handling. Both involve a node that was isolated
from the cluster for a period and then reconnects. The current sync system has no conflict
resolution and simply overwrites ‚Äî which causes entity resurrection and data loss.

---

### Scenario A ‚Äî Node offline (clean shutdown or crash, no client traffic)

The cluster is the sole source of truth. The returning node just needs to pull the current
authoritative state and discard anything that was deleted during its absence.

### Scenario B ‚Äî Network partition (node isolated but alive, serving clients)

Both sides diverge independently. Neither is authoritative. Requires bidirectional merge with
conflict resolution. This is the harder and more dangerous case.

```
Detection key:
  last_local_write_at ‚â§ last_seen_at_shutdown  ‚Üí  Scenario A (offline, no local writes)
  last_local_write_at >  last_seen_at_shutdown  ‚Üí  Scenario B (partition, had local writes)
```

---

### Phase 1 ‚Äî Schema & Detection  [ ]

**Files**: `internal/cluster/schema.go`, `internal/cluster/migration.go`

- [ ] Add column `is_stale BOOLEAN NOT NULL DEFAULT 0` to `cluster_nodes`
  - Set to `true` by health checker when a node reconnects after `> tombstone_ttl` absence
  - Set back to `false` after reconciliation completes successfully
- [ ] Add column `last_local_write_at TIMESTAMP` to `cluster_nodes`
  - Updated whenever the local node creates or modifies any entity (user, tenant, access key,
    bucket permission, IDP provider, group mapping)
  - Persisted in SQLite so it survives restarts; enables partition detection on reconnect
- [ ] Add DB migration (next migration number after current latest)
  - `ALTER TABLE cluster_nodes ADD COLUMN is_stale BOOLEAN NOT NULL DEFAULT 0`
  - `ALTER TABLE cluster_nodes ADD COLUMN last_local_write_at TIMESTAMP`

---

### Phase 2 ‚Äî Stale Detection in Health Checker  [ ]

**File**: `internal/cluster/health.go`

- [ ] In `CheckNodeHealth()`, before updating `last_seen`, capture the previous `last_seen` value
- [ ] When a node transitions from `unavailable` ‚Üí `healthy`:
  - Calculate gap = `time.Since(previousLastSeen)`
  - If gap > tombstone cleanup TTL (currently 7 days, read from `cluster_global_config`):
    set `is_stale = true` on that node record
  - Log a warning: `"node X reconnected after N days ‚Äî marked stale, reconciliation required"`
- [ ] Add helper `SetNodeStale(ctx, nodeID string, stale bool) error` in `manager.go`
- [ ] Add helper `IsNodeStale(ctx, nodeID string) (bool, error)` in `manager.go`
- [ ] Add helper `GetLocalNodeLastLocalWriteAt(ctx) (time.Time, error)` in `manager.go`
- [ ] Add helper `UpdateLastLocalWriteAt(ctx) error` in `manager.go`
  - Called from every entity create/update handler that modifies local state

---

### Phase 3 ‚Äî LWW (Last-Write-Wins) in all entity upsert handlers  [ ]

**Problem**: every internal sync endpoint currently does a blind upsert ‚Äî the last node to sync
wins regardless of which change was actually more recent. During a partition, this causes
arbitrary data loss.

**Rule**:
```
incoming.updated_at > local.updated_at  ‚Üí  accept  (remote is newer)
incoming.updated_at ‚â§ local.updated_at  ‚Üí  reject  (local is newer or equal, keep it)
```

- [ ] `POST /api/internal/cluster/sync/tenants` handler ‚Äî add LWW check before upsert
- [ ] `POST /api/internal/cluster/sync/users` handler ‚Äî add LWW check before upsert
- [ ] `POST /api/internal/cluster/sync/access-keys` handler ‚Äî add LWW check before upsert
- [ ] `POST /api/internal/cluster/sync/bucket-permissions` handler ‚Äî add LWW check before upsert
- [ ] `POST /api/internal/cluster/sync/idp-providers` handler ‚Äî add LWW check before upsert
- [ ] `POST /api/internal/cluster/sync/group-mappings` handler ‚Äî add LWW check before upsert
- [ ] All 6 handlers must return HTTP 200 with a `"skipped": true` field when rejecting a
  stale incoming entity, so the sender knows the local copy is authoritative

---

### Phase 4 ‚Äî Tombstone vs Entity timestamp comparison  [ ]

**Problem**: the current `DeletionLogSyncManager` applies tombstones unconditionally. During a
partition, an entity may have been modified on the isolated node AFTER it was deleted on the
rest of the cluster. The current code deletes it blindly ‚Äî the more recent update is lost.

**Rule**:
```
tombstone.deleted_at > entity.updated_at  ‚Üí  deletion wins  (entity was deleted after its
                                               last known modification ‚Äî correct deletion)
tombstone.deleted_at < entity.updated_at  ‚Üí  entity wins    (entity was modified/recreated
                                               AFTER the deletion ‚Äî deletion is stale, discard
                                               the tombstone)
```

**File**: `internal/cluster/deletion_log.go` (handler that receives and applies tombstones)

- [ ] When applying an incoming tombstone: query the local entity's `updated_at` first
- [ ] If `tombstone.deleted_at > entity.updated_at`: proceed with deletion (current behavior)
- [ ] If `tombstone.deleted_at < entity.updated_at`: discard tombstone, log warning
  `"tombstone for entity X discarded ‚Äî local entity was updated after deletion"`
- [ ] If entity does not exist locally: apply tombstone unconditionally (entity was already gone)

---

### Phase 5 ‚Äî State Snapshot Endpoint  [ ]

**File**: `internal/server/cluster_handlers.go`

New internal endpoint: `GET /api/internal/cluster/state-snapshot`
- Requires HMAC cluster authentication (same as all internal endpoints)
- Returns a JSON snapshot of ALL entities currently on this node, grouped by type,
  including `updated_at` per entity and all active tombstones with `deleted_at`

```json
{
  "node_id": "abc123",
  "snapshot_at": "2026-02-23T10:00:00Z",
  "tenants":      [{"id": "...", "updated_at": "..."}],
  "users":        [{"id": "...", "updated_at": "..."}],
  "access_keys":  [{"id": "...", "updated_at": "..."}],
  "bucket_perms": [{"id": "...", "updated_at": "..."}],
  "idp_providers":[{"id": "...", "updated_at": "..."}],
  "group_mappings":[{"id": "...", "updated_at": "..."}],
  "tombstones":   [{"entity_type": "...", "entity_id": "...", "deleted_at": "..."}]
}
```

- [ ] Register route `GET /api/internal/cluster/state-snapshot` in cluster routes
- [ ] Implement handler `handleGetStateSnapshot()` ‚Äî queries all 6 entity types + deletion log
- [ ] Add `fetchStateSnapshot(ctx, node *Node) (*StateSnapshot, error)` in `stale_reconciler.go`
  as the client-side function that calls this endpoint on a peer node

---

### Phase 6 ‚Äî Stale Reconciler  [ ]

**New file**: `internal/cluster/stale_reconciler.go`

```go
type ReconciliationMode int
const (
    ModeOffline   ReconciliationMode = iota  // node was down, cluster is authoritative
    ModePartition                            // node was alive but isolated, merge required
)
```

- [ ] `DetectReconciliationMode(ctx) ReconciliationMode`
  - Offline if `last_local_write_at ‚â§ last_seen_before_isolation` (no client activity during gap)
  - Partition if `last_local_write_at > last_seen_before_isolation` (had client writes during gap)

- [ ] `ReconcileOffline(ctx context.Context, peer *Node) error`
  - Fetch peer's state snapshot
  - For each entity type: delete locally any entity whose ID is NOT in peer's snapshot
    (those are entities deleted on the cluster while this node was down)
  - For each tombstone in snapshot: apply Phase 4 logic (deleted_at vs local updated_at)
  - Set `is_stale = false` on completion
  - Log summary: `"offline reconciliation complete ‚Äî N entities removed, M tombstones applied"`

- [ ] `ReconcilePartition(ctx context.Context, peer *Node) error`
  - Fetch peer's state snapshot (with `updated_at` per entity)
  - For each entity in peer's snapshot:
    - If not local ‚Üí pull and insert (peer has something we don't)
    - If local but peer's `updated_at` is newer ‚Üí pull and overwrite (LWW: peer wins)
    - If local and local's `updated_at` is newer ‚Üí keep local, push to peer on next sync cycle
  - For each tombstone in peer's snapshot: apply Phase 4 logic
  - For each local entity NOT in peer's snapshot and no matching tombstone:
    ‚Üí push to peer on next regular sync cycle (peer missed a creation)
  - Set `is_stale = false` on completion
  - Log summary: `"partition reconciliation complete ‚Äî N pulled, M kept local, K tombstones applied"`

- [ ] `RunReconciliation(ctx context.Context) error`
  - Entry point called at server startup when `is_stale = true`
  - Finds a healthy peer: `clusterManager.GetHealthyNodes(ctx)[0]`
  - Calls `DetectReconciliationMode()` ‚Üí dispatches to `ReconcileOffline` or `ReconcilePartition`
  - If no healthy peer available: log warning and skip (cannot reconcile without a peer)

---

### Phase 7 ‚Äî Integration into Server Startup  [ ]

**File**: `internal/server/server.go`

- [ ] After cluster is enabled and before starting sync managers in `Start()`:
  ```go
  if s.clusterManager.IsClusterEnabled() {
      if stale, _ := s.clusterManager.IsNodeStale(ctx); stale {
          logrus.Warn("Node is stale ‚Äî running reconciliation before starting sync")
          if err := s.staleReconciler.RunReconciliation(ctx); err != nil {
              logrus.WithError(err).Error("Reconciliation failed ‚Äî sync starting anyway")
          }
      }
  }
  ```
- [ ] Add `staleReconciler *cluster.StaleReconciler` field to `Server` struct
- [ ] Initialize `StaleReconciler` in `NewServer()` alongside other cluster components

---

### Phase 8 ‚Äî Track `last_local_write_at`  [ ]

**File**: `internal/server/console_api.go` (all entity create/update handlers)

- [ ] Call `s.clusterManager.UpdateLastLocalWriteAt(ctx)` in:
  - `handleCreateTenant`, `handleUpdateTenant`
  - `handleCreateUser`, `handleUpdateUser`
  - `handleCreateAccessKey`
  - `handleGrantBucketPermission`, `handleRevokeBucketPermission`
  - `handleCreateIDP`, `handleUpdateIDP`, `handleDeleteIDP`
  - `handleCreateGroupMapping`, `handleUpdateGroupMapping`, `handleDeleteGroupMapping`
- This ensures `last_local_write_at` reflects real client-driven writes, not sync-driven writes
- Sync-driven writes (from internal cluster endpoints) must NOT update `last_local_write_at`

---

### Phase 9 ‚Äî Tests  [ ]

**New file**: `internal/cluster/stale_reconciler_test.go`

- [ ] `TestDetectReconciliationMode_Offline` ‚Äî `last_local_write_at` before isolation
- [ ] `TestDetectReconciliationMode_Partition` ‚Äî `last_local_write_at` after isolation start
- [ ] `TestLWW_IncomingNewer` ‚Äî incoming entity with newer `updated_at` is accepted
- [ ] `TestLWW_IncomingOlder` ‚Äî incoming entity with older `updated_at` is rejected
- [ ] `TestLWW_IncomingEqual` ‚Äî equal timestamp ‚Üí keep local (no unnecessary write)
- [ ] `TestTombstoneWins` ‚Äî tombstone `deleted_at` > entity `updated_at` ‚Üí entity deleted
- [ ] `TestEntityWins` ‚Äî entity `updated_at` > tombstone `deleted_at` ‚Üí tombstone discarded
- [ ] `TestReconcileOffline_RemovesDeletedEntities` ‚Äî entities absent from snapshot are deleted
- [ ] `TestReconcilePartition_MergesCorrectly` ‚Äî LWW applied correctly across all entity types
- [ ] `TestStateSnapshotEndpoint` ‚Äî returns correct JSON structure with all entity types

---

### Summary of files to create / modify

| File | Action |
|---|---|
| `internal/cluster/schema.go` | Add `is_stale`, `last_local_write_at` columns |
| `internal/cluster/migration.go` | New migration for the 2 new columns |
| `internal/cluster/health.go` | Detect stale on reconnect, set `is_stale = true` |
| `internal/cluster/manager.go` | `SetNodeStale`, `IsNodeStale`, `UpdateLastLocalWriteAt`, `GetLocalNodeLastLocalWriteAt` |
| `internal/cluster/deletion_log.go` | Tombstone timestamp comparison before applying |
| `internal/cluster/stale_reconciler.go` | **New** ‚Äî full reconciliation logic |
| `internal/cluster/stale_reconciler_test.go` | **New** ‚Äî all unit tests |
| `internal/server/cluster_handlers.go` | New `GET /api/internal/cluster/state-snapshot` endpoint |
| `internal/server/server.go` | Startup: check stale ‚Üí run reconciler before sync managers |
| `internal/server/console_api.go` | All entity handlers call `UpdateLastLocalWriteAt` |

---

## üü¢ LOW PRIORITY

- [ ] Video tutorials and getting started guides
- [ ] Migration guides from MinIO/AWS S3
- [ ] Integration test infrastructure (multi-node cluster) for cluster/replication coverage

---

## ‚úÖ COMPLETED

### v0.9.2-beta (February 2026)
- ‚úÖ Bucket metrics under-reported under concurrent load ‚Äî replaced `UpdateBucketMetrics` OCC retry loop (5 attempts) with per-bucket `sync.Mutex` via `sync.Map`. Resolves VEEAM 4.2 GB stored / 2.21 GB shown discrepancy.
- ‚úÖ `RecalculateBucketStats` tenant prefix fix ‚Äî was scanning `obj:bucketName:` instead of `obj:tenantID/bucketName:key`, always returning 0 for tenant buckets.
- ‚úÖ Admin endpoint `POST /buckets/{bucket}/recalculate-stats` ‚Äî full BadgerDB scan to resync counters on demand.
- ‚úÖ Background stats reconciler ‚Äî goroutine that runs `RecalculateBucketStats` for all buckets every 15 minutes, 2-minute initial delay, clean shutdown on context cancellation.
- ‚úÖ Frontend dynamic refresh ‚Äî `refetchInterval: 30000` on dashboard, buckets listing, and bucket detail pages so stats update without navigation.
- ‚úÖ Removed `TestHandleTestLogOutput` ‚Äî called non-existent `server.handleTestLogOutput`, caused compilation failure.

### v0.9.1-beta (February 2026)
- ‚úÖ Tombstone-based cluster deletion sync ‚Äî new `cluster_deletion_log` table, `DeletionLogSyncManager`, tombstone checks in all upsert handlers
- ‚úÖ IDP provider and group mapping cluster sync with automatic synchronization
- ‚úÖ Delete-sync endpoints for all 6 entity types (users, tenants, access keys, bucket permissions, IDP providers, group mappings)
- ‚úÖ 36 new cluster sync tests (deletion log, IDP provider sync, group mapping sync)

### v0.8.0-beta (February 2026)
- ‚úÖ Object Filters & Advanced Search (content-type, size range, date range, tags) ‚Äî new `/objects/search` endpoint + frontend filter panel
- ‚úÖ Version check notification badge for global admins (proxied through backend)
- ‚úÖ Dark mode toggle fixed ‚Äî now uses ThemeContext, persists to user profile
- ‚úÖ CI/CD test fix ‚Äî `TestCPUStats_ConsistentData` frequency variance threshold increased for virtualized environments

### v0.7.0-beta (January-February 2026)
- ‚úÖ Bucket Inventory System
- ‚úÖ Database Migration System
- ‚úÖ Performance Profiling & Benchmarking
- ‚úÖ Cluster Production Hardening (rate limiting, circuit breakers, metrics)
- ‚úÖ ListBuckets cross-node aggregation (fixed UX blocker)
- ‚úÖ Cluster-aware quota enforcement (fixed security vulnerability)
- ‚úÖ Backend test coverage expansion ‚Äî reached practical ceiling (metadata 87.4%, object 77.3%, server 66.1%, cmd 71.4%)

### v0.6.x
- ‚úÖ Cluster Management System
- ‚úÖ Performance Metrics & Dashboards
- ‚úÖ Prometheus/Grafana Integration
- ‚úÖ Bucket Replication

### v0.5.0 and earlier
- ‚úÖ Core S3 Operations (PutObject, GetObject, DeleteObject, ListObjects)
- ‚úÖ Bucket Versioning & Lifecycle Policies
- ‚úÖ Object Lock & Retention (COMPLIANCE/GOVERNANCE)
- ‚úÖ Server-side Encryption (AES-256-CTR)
- ‚úÖ Multi-tenancy with quotas
- ‚úÖ Two-Factor Authentication
- ‚úÖ Bucket Policies, ACLs, CORS, Tags

---

## üó∫Ô∏è Long-Term Roadmap

### v0.9.0-beta (Q2 2026)
- [x] LDAP/Active Directory integration
- [x] OAuth2/OIDC SSO (Google, Microsoft presets)
- [x] Identity Provider management UI (CRUD, test connection, LDAP browser, group mappings)
- [x] External user import with role assignment (no auto-provisioning)
- [x] Group-to-role mapping with manual and automatic sync
- [x] OAuth login flow with CSRF protection and 2FA support
- [x] Auth provider badge on user list (Local/LDAP/SSO)
- [x] OAuth auto-provisioning with group mapping authorization (admin must define authorized groups)
- [x] SSO one-button-per-type login (one "Sign in with Google" button, not one per provider)
- [x] Cross-provider user lookup (search ALL OAuth providers on callback for multi-tenant routing)
- [x] SSO user creation from Users page (auth provider dropdown, conditional password)
- [x] Email auto-sync for SSO users (populated from OAuth profile on login)
- [x] Redirect URI auto-generation from PublicConsoleURL
- [x] SSO documentation (`docs/SSO.md`)
- [ ] Integration test infrastructure (multi-node, remote S3) for cluster/replication coverage
- [ ] Chaos engineering tests
- [x] Unit tests for IDP crypto, store (SQLite CRUD), manager (encrypt/decrypt/mask/cache)
- [x] Frontend tests for IDP page (list, search, delete, test connection, permissions, badges)
- [x] Unit tests for OAuth provider: presets, TestConnection, GetAuthURL, fetchUserInfo with mock HTTP, ExchangeCode error handling (24 sub-tests)
- [x] Unit tests for LDAP provider: EscapeFilter injection prevention, EntryToExternalUser attribute mapping/fallbacks, getUserAttributes, connection error handling (14 tests)
- [x] Unit tests for server IDP handlers: resolveRoleFromMappings role priority, CRUD auth/validation, OAuth callback CSRF/state, handleOAuthStart, preset deduplication, sync handlers, helpers (55+ sub-tests)
- [x] Public version endpoint (`GET /api/v1/version`) ‚Äî login page fetches version dynamically
- [x] Tombstone-based cluster deletion sync ‚Äî prevents entity resurrection in bidirectional sync
- [x] Cluster sync for IDP providers and group mappings (automatic, checksum-based skip)
- [x] Unit tests for deletion log, IDP provider sync, and group mapping sync (36 new tests)
- [x] Integration tests for IDP import/sync flows ‚Äî `TestHandleIDPImportUsers`, `TestHandleSyncGroupMapping`, `TestHandleSyncAllMappings`, plus cluster sync managers

### v1.0.0-RC (Q3 2026)
- [ ] SAML SSO
- [ ] Automatic cluster scaling
- [ ] Security audit completion

### v1.0.0 (Q4 2026)
- [ ] PostgreSQL/MySQL metadata backends
- [ ] Kubernetes operator & Helm charts
- [ ] Cloud marketplace listings

---

## üìù References

- Changelog: [CHANGELOG.md](CHANGELOG.md)
- API Documentation: [docs/API.md](docs/API.md)
- Cluster Guide: [docs/CLUSTER.md](docs/CLUSTER.md)
- Performance: [docs/PERFORMANCE.md](docs/PERFORMANCE.md)
